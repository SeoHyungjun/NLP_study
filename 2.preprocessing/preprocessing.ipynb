{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 영문 전처리 실습\n",
        " NLTK lib (https://www.nltk.org/) 사용"
      ],
      "metadata": {
        "id": "0hcs2nf4W-O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) 영문 토큰화\n",
        "https://www.nltk.org/api/nltk.tokenize.html"
      ],
      "metadata": {
        "id": "9bNtCspwXMEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk // 영문의 경우 nltk 라이브러리를 많이 사용"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwsBjIuhXKDB",
        "outputId": "12ec554e-de1a-495c-f461-4b9c13e3f178"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# workd_tokenize() : 마침표와 구두점(온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화\n",
        "import nltk\n",
        "nltk.download('punkt') # punkt(마침표와 구두점 등)을 다운로드\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Barack Obama likes fried chicken very much\"\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHijcbCQXY-i",
        "outputId": "bcaf98e2-4901-42f2-e72e-6822884b03ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "WordPunctTokenizer? # ?를 뒤에 사용하면 도움말을 볼 수 있다."
      ],
      "metadata": {
        "id": "sjESDgzBYIJu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordPunctTokenizer() : 알파벳이 아닌문자를 구분하여 토큰화\n",
        "import nltk\n",
        "from nltk import WordPunctTokenizer\n",
        "\n",
        "text = \"Barack Obama likes fried chicken very much\"\n",
        "wordpuncttoken = WordPunctTokenizer().tokenize(text)\n",
        "print(wordpuncttoken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDpXpSEbYXIH",
        "outputId": "7305594c-f07c-466f-879a-d893973f0c30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TreebankWordTokenizer() : 정규표현식에 기반한 토큰화\n",
        "import nltk\n",
        "from nltk import TreebankWordTokenizer\n",
        "\n",
        "text = \"Barack Obama likes fried chicken very much\"\n",
        "treebankwordtoken = TreebankWordTokenizer().tokenize(text)\n",
        "print(treebankwordtoken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SZvPebJY2uW",
        "outputId": "76608233-2227-47ab-ca0e-be54af5c608a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 외에도 다양한 tokenizer가 존재한다.\n",
        "내가 사용하는 데이터를 tokenize해보고 적합한것을 고르는 방식으로 사용"
      ],
      "metadata": {
        "id": "M-FEe_8WZMiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) 영문 품사 부착(PoS Tagging)\n",
        "분리한 토큰마다 품사를 부착한다.\n",
        "\n",
        "https://www.nltk.org/api/nltk.tag.html\n",
        "\n",
        "태크목록 : https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging"
      ],
      "metadata": {
        "id": "JIpi09YSZJO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMcoM04JZGGE",
        "outputId": "2f994875-b34c-43c6-d411-a65ac00219d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "taggedToken = pos_tag(word_tokens)\n",
        "print(taggedToken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Rc3du_ZxCn",
        "outputId": "74605a2e-25ed-42ed-f0e8-ab1e20bebc81"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Barack', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) 개체명 인식(NER, Named Entity Recognition)\n",
        "\n",
        "https://www.nltk.org/api/nltk.chunk.html"
      ],
      "metadata": {
        "id": "NBszvHsYZ97L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcKsAa7YZ7tg",
        "outputId": "c32848b3-b253-497e-a895-189739f98cc2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DSUD_rHaXNq",
        "outputId": "c45aa518-e2a6-4b83-d14f-f0bd6f0e59b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (ORGANIZATION Obama/NNP)\n",
            "  likes/VBZ\n",
            "  fried/VBN\n",
            "  chicken/JJ\n",
            "  very/RB\n",
            "  much/JJ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) 원형 복원\n",
        "\n",
        "각 코큰의 원형을 복원하여 표준화 한다."
      ],
      "metadata": {
        "id": "nnHE64FGaljf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4-1) 어간추출 (Stemming)\n",
        "* 규칙에 기반 하여 토큰을 표준화\n",
        "* ning 제거, ful 제거 등\n",
        "\n",
        "https://www.nltk.org/api/nltk.stem.html\n",
        "\n",
        "규칙상세 : https://tartarus.org/martin/PorterStemmer/def.txt"
      ],
      "metadata": {
        "id": "iVWRY46naqoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "print(\"running -> \" + ps.stem(\"running\"))\n",
        "print(\"beautiful -> \" + ps.stem(\"beautiful\"))\n",
        "print(\"believes -> \" + ps.stem(\"believes\"))\n",
        "print(\"using -> \" + ps.stem(\"using\"))\n",
        "print(\"conversation -> \" + ps.stem(\"conversation\"))\n",
        "print(\"organization -> \" + ps.stem(\"organization\"))\n",
        "print(\"studies -> \" + ps.stem(\"studies\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4t3kvtwadLd",
        "outputId": "3b3bce74-dd3a-47b5-f52e-0b3ad769928b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "beautiful -> beauti\n",
            "believes -> believ\n",
            "using -> use\n",
            "conversation -> convers\n",
            "organization -> organ\n",
            "studies -> studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "beautiful같은 경우 beauty가 아닌 사용하지 않는 단어인 beauti가 나온다."
      ],
      "metadata": {
        "id": "B-CZdC2JbeOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4-2)표제어 추출 (Lemmatization)\n",
        "* 품사정보를 보존하여 토큰을 표준화\n",
        "\n",
        "http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer"
      ],
      "metadata": {
        "id": "hxTxLy9abozy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E5BqTRrbabd",
        "outputId": "7123f4f2-bfc8-4e90-e9e9-f1b1b21edbf8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
        "print(\"beautiful -> \" + wl.lemmatize(\"beautiful\"))\n",
        "print(\"believes -> \" + wl.lemmatize(\"believes\"))\n",
        "print(\"using -> \" + wl.lemmatize(\"using\"))\n",
        "print(\"conversation -> \" + wl.lemmatize(\"conversation\"))\n",
        "print(\"organization -> \" + wl.lemmatize(\"organization\"))\n",
        "print(\"studies -> \" + wl.lemmatize(\"studies\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5SeIN3xb7EL",
        "outputId": "a76a920a-ac1a-4142-fdbc-d2bba6a2304c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> running\n",
            "beautiful -> beautiful\n",
            "believes -> belief\n",
            "using -> using\n",
            "conversation -> conversation\n",
            "organization -> organization\n",
            "studies -> study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "복원한 단어가 실제 있는 단어이고, 사용할 수 있는 형태로 나왔다.\n",
        "그러나 사전에 정의되어있지 않은 단어의 경우 사용할 수 없다."
      ],
      "metadata": {
        "id": "4bY156ukcLd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) 불용어 처리 (Stopword)"
      ],
      "metadata": {
        "id": "8GlfsN51cUpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ', 'VBP'] # 품사 목록 을 통해 제거"
      ],
      "metadata": {
        "id": "Evd4o3GXcGaU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최빈어 조회\n",
        "# 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(taggedToken).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9nuD2UAcin0",
        "outputId": "7fa66590-a1b5-42a1-d75f-6032821c41f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('Barack', 'NNP'), 1),\n",
              " (('Obama', 'NNP'), 1),\n",
              " (('likes', 'VBZ'), 1),\n",
              " (('fried', 'VBN'), 1),\n",
              " (('chicken', 'JJ'), 1),\n",
              " (('very', 'RB'), 1),\n",
              " (('much', 'JJ'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopWord = [',', 'be', 'able', 'very'] # 불용어 목록\n",
        "\n",
        "word = []\n",
        "for tag in taggedToken:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZkYdtmncrs2",
        "outputId": "9c9c181e-8907-4e52-e324-7ba68065604b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barack', 'Obama', 'fried', 'chicken', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 한글 전처리 실습\n",
        "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화 해야한다."
      ],
      "metadata": {
        "id": "s5c1sBfwdTNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) 한글 토큰화 및 형태소 분석"
      ],
      "metadata": {
        "id": "vcRY0W4jdYtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# konlpy 설치\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhM9hEi4dD5C",
        "outputId": "a0598cd0-f997-435e-c994-8a3c014c50cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 600 kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 51.4 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한글 자연어처리기 비교\n",
        "\n",
        "https://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221337575742"
      ],
      "metadata": {
        "id": "jKV-5wyrdmF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코모란 (Komoran) 토큰화\n",
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
        "komoran_tokens = komoran.morphs(kor_text)\n",
        "print(komoran_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzUK9JSadePF",
        "outputId": "d9bc2fdb-364a-46b4-cae8-767a7646c08d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한나눔 (Hannanum) 토큰화\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
        "hannanum_tokens = hannanum.morphs(kor_text)\n",
        "print(hannanum_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiEsloYbeAvd",
        "outputId": "dcc3b699-b1ee-4acc-eb36-50d0906fe646"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt 토큰화\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
        "okt_tokens = okt.morphs(kor_text)\n",
        "print(okt_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEPkBFvieRx3",
        "outputId": "ada636f1-80fb-40d5-96e8-41072fc39003"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '깨닫지', '못', '하고', '인간', '과', '대화', '를', '계속', '할', '수', '있다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '될', '수', '있습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Kkma 토큰화\n",
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
        "kkma_tokens = kkma.morphs(kor_text)\n",
        "print(kkma_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhVubtOEedoG",
        "outputId": "f15a74fa-0488-4b2d-cb40-2a6bd3cb8909"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) 한글 품사 부착 (PoS Tagging)\n",
        "PoS Tag 목록\n",
        "\n",
        "https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
      ],
      "metadata": {
        "id": "ROoBEo2aewXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코모란(Komoran) 품사 태깅\n",
        "komoranTag = []\n",
        "for token in komoran_tokens:\n",
        "  komoranTag += komoran.pos(token)\n",
        "print(komoranTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ6fhz1Len0k",
        "outputId": "15540489-113b-49bc-d883-23fafe9d41ca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'MM'), ('컴퓨터', 'NNG'), ('오', 'VV'), ('아', 'EC'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'MM'), ('있', 'VV'), ('달', 'VV'), ('는', 'ETM'), ('것', 'NNB'), ('을', 'NNG'), ('깨닫', 'VV'), ('지', 'NNB'), ('못', 'MAG'), ('하', 'MAG'), ('고', 'MM'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'JKO'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('지능', 'NNP'), ('적', 'NNB'), ('이', 'MM'), ('ㄴ', 'JX'), ('것', 'NNB'), ('으로', 'JKB'), ('간주', 'NNG'), ('되', 'NNB'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('습니다', 'EC'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한나눔(Hannanum) 품사 태깅\n",
        "hannanumTag = []\n",
        "for token in hannanum_tokens:\n",
        "  hannanumTag += hannanum.pos(token)\n",
        "print(hannanumTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCer9dUVfUAX",
        "outputId": "356e5393-1778-40b4-ca65-400eb83a6c34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'N'), ('이', 'M'), ('컴퓨터', 'N'), ('와', 'I'), ('대화', 'N'), ('하', 'P'), ('고', 'E'), ('있', 'N'), ('다', 'M'), ('는', 'J'), ('것', 'N'), ('을', 'N'), ('깨닫', 'N'), ('지', 'N'), ('못하', 'P'), ('어', 'E'), ('고', 'M'), ('인간', 'N'), ('과', 'N'), ('대화', 'N'), ('를', 'N'), ('계속', 'M'), ('하', 'I'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('다면', 'N'), ('컴퓨터', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('지능적', 'N'), ('이', 'M'), ('ㄴ', 'N'), ('것', 'N'), ('으', 'N'), ('로', 'J'), ('간주', 'N'), ('되', 'N'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('슬', 'P'), ('ㅂ니다', 'E'), ('.', 'S')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt 품사 태깅\n",
        "oktTag = []\n",
        "for token in okt_tokens:\n",
        "  oktTag += okt.pos(token)\n",
        "print(oktTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTcHWaPtfc6k",
        "outputId": "09f1efb6-6c49-4021-cb0e-503bbea190c1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'Noun'), ('이', 'Noun'), ('컴퓨터', 'Noun'), ('와', 'Verb'), ('대화', 'Noun'), ('하고', 'Verb'), ('있다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('깨닫지', 'Verb'), ('못', 'Noun'), ('하고', 'Verb'), ('인간', 'Noun'), ('과', 'Noun'), ('대화', 'Noun'), ('를', 'Noun'), ('계속', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있다면', 'Adjective'), ('컴퓨터', 'Noun'), ('는', 'Verb'), ('지능', 'Noun'), ('적', 'Noun'), ('인', 'Noun'), ('것', 'Noun'), ('으로', 'Josa'), ('간주', 'Noun'), ('될', 'Verb'), ('수', 'Noun'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kkma 품사 태깅\n",
        "kkmaTag = []\n",
        "for token in kkma_tokens:\n",
        "  kkmaTag += kkma.pos(token)\n",
        "print(kkmaTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmADGly5flG9",
        "outputId": "774b2f0b-8830-40b1-c60e-f2c6fe6f9ae0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'NNG'), ('컴퓨터', 'NNG'), ('오', 'VA'), ('아', 'ECS'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'NNG'), ('있', 'VA'), ('달', 'VV'), ('는', 'ETD'), ('것', 'NNB'), ('을', 'NNG'), ('깨닫', 'VV'), ('지', 'NNG'), ('못하', 'VX'), ('고', 'NNG'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'UN'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('지능', 'NNG'), ('적', 'NNG'), ('이', 'NNG'), ('ㄴ', 'NNG'), ('것', 'NNB'), ('으', 'UN'), ('로', 'JKM'), ('간주', 'NNG'), ('되', 'VA'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('슬', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) 불용어 (Stopword) 처리\n",
        "분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다."
      ],
      "metadata": {
        "id": "zWu8PNrPfvKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 불용어 처리\n",
        "stopPos = ['Suffix', 'Punctuation', 'Josa', 'Foreign', 'Alpha', 'Number']"
      ],
      "metadata": {
        "id": "dy0pjm2Tfp39"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "GhnwDIKFgBXy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopWord = ['의','이','로','두고','들','를','은','과','수','했다','것','있는','한다','하는','그','있다','할','이런','되기','해야','있게','여기']"
      ],
      "metadata": {
        "id": "iV3JEqLLgJEK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = []\n",
        "for tag in oktTag:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf7fwQR-hJqC",
        "outputId": "d1aa7e5e-fb79-4229-a5eb-e75ffc53dc90"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '컴퓨터', '와', '대화', '하고', '있다는', '깨닫지', '못', '하고', '인간', '대화', '계속', '있다면', '컴퓨터', '는', '지능', '적', '인', '간주', '될', '있습니다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6SmaYLSjhZOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}